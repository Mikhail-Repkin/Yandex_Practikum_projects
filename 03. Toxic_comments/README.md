# Классификация токсичных комментариев

## Инструменты и библиотеки:
![Python](https://img.shields.io/badge/-Python-white?style=flat&logo=python)
![Pandas](https://img.shields.io/badge/-Pandas-white?style=flat&logo=pandas&logoColor=130754)
![Nltk](https://img.shields.io/badge/-Nltk-white?style=flat&logo=Nltk)
![Spacy](https://img.shields.io/badge/-Spacy-white?style=flat&logo=Spacy)
![tf-idf](https://img.shields.io/badge/-tf_idf-white?style=flat&logo=tf-idf)
![CatBoost](https://img.shields.io/badge/-CatBoost-white?style=flat&logo=CatBoost)
![BERT](https://img.shields.io/badge/-BERT-white?style=flat&logo=BERT)
## Описание проекта:
Интернет-магазин «Викишоп» запускает новый сервис. Пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других.

**Цель** — магазину «Викишоп» нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.

**Задача исследования** — построить модель машинного обучения позволяющую классифицировать комментарии на позитивные и негативные.

### Описание данных:
*	`text` — текст комментария;
*	`toxic` — целевой признак.
## Выводы:
В ходе работы над проектом:

* Загружены, изучены и подготовлены данные — проведен downsampling downsampling и выполнен расчет статистической меры TF-IDF;
* Данные были разделены на обучающую и тестовую выборки;
* Обучены четыре разные модели и выбрана лучшая на валидационной выборке;
* Согласно метрике F1 лучше всего себя показала модель CatBoost (learning_rate: 0.2) с векторизацией данных TF-IDF: F1 = 0.778 на тестовой выборке.

**Таким образом, были протестированы различные модели машинного обучения и выбрана целевая перспективная модель (CatBoost) позволяющая классифицировать комментарии на позитивные и негативные.**
