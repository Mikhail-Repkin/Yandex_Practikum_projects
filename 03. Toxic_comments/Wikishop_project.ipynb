{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490366d9",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Описание-данных\" data-toc-modified-id=\"Описание-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Описание данных</a></span></li><li><span><a href=\"#Обработка-пропусков\" data-toc-modified-id=\"Обработка-пропусков-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Обработка пропусков</a></span></li><li><span><a href=\"#Поиск-и-обработка-дубликатов\" data-toc-modified-id=\"Поиск-и-обработка-дубликатов-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Поиск и обработка дубликатов</a></span></li><li><span><a href=\"#Проверка-данных-на-дисбаланс-классов\" data-toc-modified-id=\"Проверка-данных-на-дисбаланс-классов-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Проверка данных на дисбаланс классов</a></span></li><li><span><a href=\"#Тестирование-метода-борьбы-с-дисбалансом-на-модели-логистической-регрессии\" data-toc-modified-id=\"Тестирование-метода-борьбы-с-дисбалансом-на-модели-логистической-регрессии-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Тестирование метода борьбы с дисбалансом на модели логистической регрессии</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Выводы</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Определение-параметров-и-функций\" data-toc-modified-id=\"Определение-параметров-и-функций-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Определение параметров и функций</a></span></li><li><span><a href=\"#Модель-LogisticRegression\" data-toc-modified-id=\"Модель-LogisticRegression-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Модель LogisticRegression</a></span></li><li><span><a href=\"#Модель-RandomForest\" data-toc-modified-id=\"Модель-RandomForest-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Модель RandomForest</a></span></li><li><span><a href=\"#Модель-CatBoost\" data-toc-modified-id=\"Модель-CatBoost-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Модель CatBoost</a></span></li><li><span><a href=\"#Тестирование-лучшей-модели-с-векторизацией-данных-TF-IDF\" data-toc-modified-id=\"Тестирование-лучшей-модели-с-векторизацией-данных-TF-IDF-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Тестирование лучшей модели с векторизацией данных TF-IDF</a></span></li><li><span><a href=\"#Модель-BERT\" data-toc-modified-id=\"Модель-BERT-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Модель BERT</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Выводы</a></span></li></ul></li><li><span><a href=\"#Общий-вывод\" data-toc-modified-id=\"Общий-вывод-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Общий вывод</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56d2d1",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da0b80",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других.\n",
    "\n",
    "**Цель** — магазину «Викишоп» нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "\n",
    "**Задача исследования** — построить модель машинного обучения позволяющую классифицировать комментарии на позитивные и негативные.\n",
    "\n",
    "**Исследование пройдёт в три этапа:**\n",
    " 1. Загрузка и подготовка данных;\n",
    " 2. Обучение и тестирование моделей;\n",
    " 3. Выводы.\n",
    " \n",
    "*Примечания:* \n",
    "* Для оценки качества моделей используется метрика F1. Значение метрики F1 на тестовой выборке должно быть не меньше 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcf26a",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef6e2999",
   "metadata": {},
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1c899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\m1kha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\m1kha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\m1kha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\m1kha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 9.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (63.4.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.1)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1kha\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2022-09-29 10:22:11.706926: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-09-29 10:22:11.706952: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-29 10:22:11.709002: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-9K9TKS6\n",
      "2022-09-29 10:22:11.709048: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-9K9TKS6\n"
     ]
    }
   ],
   "source": [
    "# импорт библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import Pool, CatBoostClassifier, cv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "from tqdm import notebook\n",
    "from pymystem3 import Mystem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7d483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сброс ограничений на количество выводимых рядов\n",
    "pd.set_option('display.max_rows', None)\n",
    " \n",
    "# сброс ограничений на число столбцов\n",
    "pd.set_option('display.max_columns', None)\n",
    " \n",
    "# сброс ограничений на количество символов в записи\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896d910",
   "metadata": {},
   "source": [
    "Данные для исследования получаем из файла `toxic_comments`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6049f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение файлов с данными и сохранение в df\n",
    "try:\n",
    "    df = pd.read_csv('datasets/toxic_comments.csv', index_col=0)\n",
    "except:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e004a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# получение первых пяти строк таблицы\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6d2e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# получение общей информации о данных\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd065344",
   "metadata": {},
   "source": [
    "### Описание данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65172b",
   "metadata": {},
   "source": [
    "*\t`text` — текст комментария;\n",
    "*\t`toxic` — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6e650",
   "metadata": {},
   "source": [
    "###  Обработка пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d751dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0.0\n",
       "toxic    0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# подсчёт доли пропусков \n",
    "display(df.isna().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf1c22",
   "metadata": {},
   "source": [
    "В данных не содержатся пропуски."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83935e1",
   "metadata": {},
   "source": [
    "### Поиск и обработка дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a02540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Явных дубликатов в df: 0\n"
     ]
    }
   ],
   "source": [
    "# подсчет явных дубликатов\n",
    "print(\"Явных дубликатов в df:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ba0e9",
   "metadata": {},
   "source": [
    "В данных не содержится дубликатов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce9a7d",
   "metadata": {},
   "source": [
    "### Проверка данных на дисбаланс классов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd91f3",
   "metadata": {},
   "source": [
    "Оценим баланс классов целевого признака в столбце `toxic`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec13e433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# подсчет количества значений целевого признака\n",
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5229363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля положительные значений: 10.16%\n",
      "Доля отрицательных значений: 89.84%\n"
     ]
    }
   ],
   "source": [
    "# подсчет доли положительных/отрицательных значений\n",
    "print('Доля положительные значений: {:.2%}'.format(df['toxic'].mean()))\n",
    "print('Доля отрицательных значений: {:.2%}'.format(1-df['toxic'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c21c19",
   "metadata": {},
   "source": [
    "Присутствует явное смещение в сторону отрицательных значений.    \n",
    "\n",
    "Метод увеличения выборки (upsampling) не имеет смысла использовать ввиду изначально большого количества данных. С другой стороны, так как выборка сильно несбалансирована, то применяя метод уменьшения выборки (downsampling) мы лишимся большого количества данных, важных для обучения, что скажется на итоговом качестве модели.    \n",
    "\n",
    "Поэтому, принимаем решение в качестве метода борьбы с дисбалансом классов использовать взвешивание (balanced)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602ee0b",
   "metadata": {},
   "source": [
    "### Тестирование метода борьбы с дисбалансом на модели логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e038f",
   "metadata": {},
   "source": [
    "Протестируем выбранный метод борьбы с дисбалансом на модели логистической регрессии. В качестве определяющей метрики будем использовать F1-меру."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45235773",
   "metadata": {},
   "source": [
    "**Лемматизация данных, подготовка признаков**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91fe56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция лемматизации и очистки символов\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def lemmatize(text):\n",
    "    text = text.lower()\n",
    "    lemm_list = nlp(text) #лемматизация\n",
    "    lemm_text = \" \".join([token.lemma_ for token in lemm_list]) #объединяем элементы списка в строку, разделив их пробелом\n",
    "    text_clear = re.sub(r'[^a-zA-Z]', ' ', lemm_text) #оставляем в тексте только латинские символы\n",
    "    lemm_text_clear = \" \".join(text_clear.split()) #объединяем элементы списка в строку, удалив лишние пробелом\n",
    "    \n",
    "    return lemm_text_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60f1a9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 23min 14s\n",
      "Wall time: 23min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['lemm_text'] = df['text'].apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49eeb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Исходный и лемматизированный текст:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edit make under my username hardcore metallica fan be revert they be not vandalism just closure on some gas after I vote at new york dolls fac and please do not remove the template from the talk page since I be retire now</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                        text  \\\n",
       "0  Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "\n",
       "   toxic  \\\n",
       "0      0   \n",
       "\n",
       "                                                                                                                                                                                                                                           lemm_text  \n",
       "0  explanation why the edit make under my username hardcore metallica fan be revert they be not vandalism just closure on some gas after I vote at new york dolls fac and please do not remove the template from the talk page since I be retire now  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"Исходный и лемматизированный текст:\", df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c375499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#удаление исходного корпуса текстов\n",
    "df = df.drop(['text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60069383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создание выборки с признаками и целевыми признаками\n",
    "features = df.drop('toxic', axis=1)\n",
    "target = df['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf754c",
   "metadata": {},
   "source": [
    "**Разделение данных на выборки**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2ed1d",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую (80%) и тестовую (20%) выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff8dc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделение данных на обучающую и тестовую выборки\n",
    "features_train, features_test = train_test_split(features.copy(), train_size=0.8,\n",
    "                                                 test_size=0.2, random_state=12345)\n",
    "target_train, target_test = train_test_split(target.copy(), train_size=0.8,\n",
    "                                             test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f33d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент данных в обучающей выборке признаков: 80%\n",
      "Процент данных в тестовой выборке признаков 20%\n",
      "\n",
      "Процент данных в обучающей выборке целевых признаков: 80%\n",
      "Процент данных в тестовой выборке целевых признаков 20%\n"
     ]
    }
   ],
   "source": [
    "print(\"Процент данных в обучающей выборке признаков:\", '{:.0%}'.format(len(features_train)/len(features)))\n",
    "print(\"Процент данных в тестовой выборке признаков\", '{:.0%}'.format(len(features_test)/len(features)))\n",
    "print()\n",
    "print(\"Процент данных в обучающей выборке целевых признаков:\", '{:.0%}'.format(len(target_train)/len(target)))\n",
    "print(\"Процент данных в тестовой выборке целевых признаков\", '{:.0%}'.format(len(target_test)/len(target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e92d3a",
   "metadata": {},
   "source": [
    "**Пайплайн для моделей и оценки TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25519b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#стоп-слова\n",
    "stopwords = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0265d2",
   "metadata": {},
   "source": [
    "Создадим пайплайн, где соединим подсчет величины TF-IDF для корпуса текстов и модели обучения: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c93d05bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#пайплан TF-IDF и модель логистической регрессии\n",
    "model_logit_unbalanced = make_pipeline(CountVectorizer(stop_words=stopwords), TfidfTransformer(),\n",
    "                            LogisticRegression(random_state=12345, max_iter=300))\n",
    "\n",
    "model_logit_balanced = make_pipeline(CountVectorizer(stop_words=stopwords), TfidfTransformer(),\n",
    "                            LogisticRegression(random_state=12345, max_iter=300, class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3767ae4",
   "metadata": {},
   "source": [
    "**Обучение модели без баланса классов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80369c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.732\n",
      "\n",
      "CPU times: total: 828 ms\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# обучение модели\n",
    "f1_unbalanced = cross_val_score(model_logit_unbalanced, features_train['lemm_text'].values,\n",
    "                                target_train, cv=5, scoring='f1', n_jobs=-1).mean()\n",
    "\n",
    "# вывод результатов\n",
    "print('F1:', round(f1_unbalanced, 3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1aa60",
   "metadata": {},
   "source": [
    "**Обучение модели с взвешенными классами**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feca69d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.747\n",
      "\n",
      "CPU times: total: 562 ms\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# обучение модели\n",
    "f1_balanced = cross_val_score(model_logit_balanced, features_train['lemm_text'].values,\n",
    "                              target_train, cv=5, scoring='f1', n_jobs=-1).mean()\n",
    "\n",
    "# вывод результатов\n",
    "print('F1:', round(f1_balanced, 3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a130b",
   "metadata": {},
   "source": [
    "**Итоги:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e2df163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unbalanced</th>\n",
       "      <th>Balanced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.732</td>\n",
       "      <td>0.747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unbalanced  Balanced\n",
       "F1       0.732     0.747"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# итоги обучения моделей\n",
    "results_balancing = pd.DataFrame({'Unbalanced': f1_unbalanced, 'Balanced': f1_balanced},\n",
    "                                 index=['F1'])\n",
    "display(round(results_balancing, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e9ed6",
   "metadata": {},
   "source": [
    "💡 Присутствует некоторое улучшение в целевой метрике, поэтому будем использовать взвешивание классов для баласировки выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6b616",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade44f98",
   "metadata": {},
   "source": [
    "💡 В исходном файле содержится набор данных с комментариями пользователей сервиса размеченный по токсичности правок. \n",
    "\n",
    "В ходе первичного анализа и предобработки данных было выполнено:     \n",
    "* В данных не выявлены пропуски или дубликаты;   \n",
    "\n",
    "* Данные сильно не сбалансированы, поэтому будем применять \"взвешивание классов\" (class_weight='balanced');    \n",
    "\n",
    "* Был выполнен расчет статистической меры TF-IDF, используемой для оценки важности слова в контексте корпуса текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a345a3",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d457601",
   "metadata": {},
   "source": [
    "### Определение параметров и функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "337089a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_logit = {'logisticregression__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "                   'logisticregression__penalty': ['l2', 'none']}\n",
    "\n",
    "parameters_RF = {'randomforestclassifier__max_depth': list(range(10, 221, 30))\n",
    "                }\n",
    "                 \n",
    "parameters_CB = {'catboostclassifier__learning_rate': [0.1, 0.2]\n",
    "                }\n",
    "\n",
    "model_logit = make_pipeline(CountVectorizer(stop_words=stopwords), TfidfTransformer(),\n",
    "                            LogisticRegression(random_state=12345, max_iter=2000, class_weight='balanced'))\n",
    "\n",
    "model_RF =  make_pipeline(CountVectorizer(stop_words=stopwords), TfidfTransformer(),\n",
    "                          RandomForestClassifier(random_state=12345, class_weight='balanced'))\n",
    "\n",
    "model_CB = make_pipeline(CountVectorizer(stop_words=stopwords), TfidfTransformer(),\n",
    "                         CatBoostClassifier(random_state=12345, verbose = False))\n",
    "\n",
    "cv = 3 #количество итераций при кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1227a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv(model, features_train, target_train, parameters):\n",
    "       \n",
    "    grid = GridSearchCV(estimator=model, param_grid=parameters, cv=cv, scoring='f1',\n",
    "                        n_jobs=-1, error_score='raise')\n",
    "    \n",
    "    grid.fit(features_train, target_train)\n",
    "    \n",
    "    F1 = grid.best_score_\n",
    "    best_params = grid.best_params_\n",
    "    \n",
    "    print(f'F1:', round(F1, 3))\n",
    "    print(\"Параметры лучшей модели:\", best_params)\n",
    "    print()\n",
    "    \n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6c7eb",
   "metadata": {},
   "source": [
    "### Модель LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9bee1",
   "metadata": {},
   "source": [
    "Для ускорения поиска наилучших гиперпараметров выполним кросс-валидацию на части выборки (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63953b63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.712\n",
      "Параметры лучшей модели: {'logisticregression__penalty': 'l2', 'logisticregression__solver': 'lbfgs'}\n",
      "\n",
      "CPU times: total: 3.12 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#обучение модели\n",
    "best_params_logit = grid_search_cv(model_logit, features_train['lemm_text'].sample(frac=0.2, random_state=12345).values,\n",
    "                                   target_train.sample(frac=0.2, random_state=12345), parameters_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f4746",
   "metadata": {},
   "source": [
    "### Модель RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77cc3ab",
   "metadata": {},
   "source": [
    "Для ускорения поиска наилучших гиперпараметров выполним кросс-валидацию на части выборки (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98664464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.592\n",
      "Параметры лучшей модели: {'randomforestclassifier__max_depth': 220}\n",
      "\n",
      "CPU times: total: 23.1 s\n",
      "Wall time: 49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#обучение модели\n",
    "best_params_RF = grid_search_cv(model_RF, features_train['lemm_text'].sample(frac=0.2, random_state=12345).values,\n",
    "                                target_train.sample(frac=0.2, random_state=12345), parameters_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407d991",
   "metadata": {},
   "source": [
    "### Модель CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57918d5",
   "metadata": {},
   "source": [
    "Для ускорения поиска наилучших гиперпараметров выполним кросс-валидацию на части выборки (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7798ef27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.718\n",
      "Параметры лучшей модели: {'catboostclassifier__learning_rate': 0.2}\n",
      "\n",
      "CPU times: total: 21min 40s\n",
      "Wall time: 12min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#обучение модели\n",
    "best_params_CB = grid_search_cv(model_CB, features_train['lemm_text'].sample(frac=0.2, random_state=12345).values,\n",
    "                                target_train.sample(frac=0.2, random_state=12345), parameters_CB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228eddad",
   "metadata": {},
   "source": [
    "### Тестирование лучшей модели с векторизацией данных TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1091fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#пайплайн векторизации данных TF-IDF\n",
    "pipe = make_pipeline(CountVectorizer(stop_words=stopwords), TfidfTransformer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee4be6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей матрицы: (127433, 134106)\n",
      "Размер тестовой матрицы: (31859, 134106)\n"
     ]
    }
   ],
   "source": [
    "#подсчет величины TF-IDF для корпуса текстов обучающей и тестовой выборок\n",
    "features_train_TF_IDF = pipe.fit_transform(features_train['lemm_text'].values)\n",
    "features_test_TF_IDF = pipe.transform(features_test['lemm_text'].values)\n",
    "print(\"Размер обучающей матрицы:\", features_train_TF_IDF.shape)\n",
    "print(\"Размер тестовой матрицы:\", features_test_TF_IDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7024966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.778\n",
      "\n",
      "CPU times: total: 1h 9min 11s\n",
      "Wall time: 9min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# обучение модели\n",
    "model = CatBoostClassifier(random_state=12345, learning_rate = 0.2, verbose = False)\n",
    "model.fit(features_train_TF_IDF, target_train)\n",
    "predictions = model.predict(features_test_TF_IDF)\n",
    "\n",
    "f1 = f1_score(target_test, predictions)\n",
    "\n",
    "# вывод результатов\n",
    "print('F1:', round(f1, 3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3be09",
   "metadata": {},
   "source": [
    "### Модель BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0aa38",
   "metadata": {},
   "source": [
    "Обучение и тестирование модели BERT выполнено в Google Colab по ссылке ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719712dd",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1zNSP92uxpKoXEs-SZxgFu31VfupaWmQ5?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c09ee",
   "metadata": {},
   "source": [
    "Поиск наилучших гиперпараметров проведен 10% выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f47c19ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на обучающей выборке: 0.666\n",
      "Параметры лучшей модели: C = 0.5\n"
     ]
    }
   ],
   "source": [
    "print('F1 на обучающей выборке:', 0.666)\n",
    "print('Параметры лучшей модели:', 'C = 0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e525ca",
   "metadata": {},
   "source": [
    "Результаты обучения на тестовой выборке с взвешенными классами и силой регуляризации 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66d58dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на тестовой выборке: 0.666\n"
     ]
    }
   ],
   "source": [
    "print('F1 на тестовой выборке:', 0.666)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3024cc",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac5db0",
   "metadata": {},
   "source": [
    "Кросс-валидадационный поиск лучшей модели на обучающей выборке дал следующие результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98c58bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>BERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.712</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LogisticRegression  RandomForest  CatBoost   BERT\n",
       "F1               0.712         0.592     0.718  0.666"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# итоги обучения моделей\n",
    "results = pd.DataFrame({'LogisticRegression': best_params_logit, 'RandomForest': best_params_RF,\n",
    "                        'CatBoost': best_params_CB, 'BERT': 0.666}, index=['F1'])\n",
    "display(round(results, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d85bf",
   "metadata": {},
   "source": [
    "Обучение на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e77f183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>BERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.778</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CatBoost   BERT\n",
       "F1     0.778  0.666"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# итоги обучения моделей\n",
    "results = pd.DataFrame({'CatBoost': f1, 'BERT': 0.666}, index=['F1'])\n",
    "display(round(results, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dde0ce",
   "metadata": {},
   "source": [
    "Таким образом модель CatBoost с векторизацией данных TF-IDF показала наилучшие результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b59826",
   "metadata": {},
   "source": [
    "## Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a5baa",
   "metadata": {},
   "source": [
    "В ходе работы было выполнено:\n",
    "\n",
    "* Загружены, изучены и подготовлены данные — проведен downsampling downsampling и выполнен расчет статистической меры TF-IDF;\n",
    "* Данные были разделены на обучающую и тестовую выборки;\n",
    "* Обучены четыре разные модели и выбрана лучшая на валидационной выборке;\n",
    "* Согласно метрике F1 лучше всего себя показала модель CatBoost с векторизацией данных TF-IDF: F1 = 0.778 на тестовой выборке.\n",
    "\n",
    "**Таким образом, были протестированы различные модели машинного обучения и выбрана целевая перспективная модель (CatBoost) позволяющая классифицировать комментарии на позитивные и негативные.**"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 6780,
    "start_time": "2022-09-25T20:11:15.068Z"
   },
   {
    "duration": 3,
    "start_time": "2022-09-25T20:11:21.850Z"
   },
   {
    "duration": 3269,
    "start_time": "2022-09-25T20:11:21.855Z"
   },
   {
    "duration": 9,
    "start_time": "2022-09-25T20:11:25.125Z"
   },
   {
    "duration": 31,
    "start_time": "2022-09-25T20:11:25.136Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "455px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
